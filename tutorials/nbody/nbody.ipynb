{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Body Experiment\n",
    "The goal of this notebook is to demonstrate how a workflow can be simplified using equiadapt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from equiadapt.nbody.canonicalization.euclidean_group import EuclideanGroupNBody\n",
    "from equiadapt.nbody.canonicalization_networks.custom_equivariant_networks import VNDeepSets\n",
    "from equiadapt.common.utils import gram_schmidt\n",
    "\n",
    "from examples.nbody.networks.euclideangraph_base_models import GNN\n",
    "from examples.nbody.prepare.nbody_data import NBodyDataModule\n",
    "from examples.nbody.model_utils import get_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameters:\n",
    "    def __init__(self):\n",
    "        self.model = \"NBodyPipeline\"\n",
    "        self.canon_model_type = \"vndeepsets\"\n",
    "        self.pred_model_type = \"Transformer\"\n",
    "        self.batch_size = 100\n",
    "        self.dryrun = False\n",
    "        self.use_wandb = False\n",
    "        self.checkpoint = False\n",
    "        self.num_epochs = 1000\n",
    "        self.num_workers = 0\n",
    "        self.auto_tune = False\n",
    "        self.seed = 0\n",
    "        self.learning_rate = 1e-3\n",
    "        self.weight_decay = 1e-12\n",
    "        self.patience = 1000\n",
    "\n",
    "class CanonicalizationHyperparameters:\n",
    "    def __init__(self):\n",
    "        self.architecture = \"vndeepsets\"\n",
    "        self.num_layers = 4\n",
    "        self.hidden_dim = 16\n",
    "        self.layer_pooling = \"mean\"\n",
    "        self.final_pooling = \"mean\"\n",
    "        self.out_dim = 4\n",
    "        self.batch_size = 100\n",
    "        self.nonlinearity = \"relu\"\n",
    "        self.canon_feature = \"p\"\n",
    "        self.canon_translation = False\n",
    "        self.angular_feature = \"pv\"\n",
    "        self.dropout = 0.5\n",
    "\n",
    "class PredictionHyperparameters:\n",
    "    def __init__(self):\n",
    "        self.architecture = \"GNN\"\n",
    "        self.num_layers = 4\n",
    "        self.hidden_dim = 32\n",
    "        self.input_dim = 6\n",
    "        self.in_node_nf = 1\n",
    "        self.in_edge_nf = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = Hyperparameters()\n",
    "canon_hyperparams = CanonicalizationHyperparameters()\n",
    "pred_hyperparams = PredictionHyperparameters()\n",
    "hyperparams.canon_hyperparams = canon_hyperparams\n",
    "hyperparams.pred_hyperparams = pred_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbody_data = NBodyDataModule(hyperparams)\n",
    "nbody_data.setup()\n",
    "train_loader = nbody_data.train_dataloader()\n",
    "\n",
    "nbody_data.setup(stage=\"test\")\n",
    "test_loader = nbody_data.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the batch into location features, velocity features, \n",
    "# node features, edges, edge features, charges, and end locations (ie. targets)\n",
    "def get_data(batch):\n",
    "    batch_size, n_nodes, _ = batch[0].size()\n",
    "    batch = [d.view(-1, d.size(2)) for d in batch]  # converts to 2D matrices\n",
    "    loc, vel, edge_attr, charges, loc_end = batch\n",
    "    edges = get_edges(\n",
    "        batch_size, n_nodes\n",
    "    )  # returns a list of two tensors, each of size num_edges * batch_size (where num_edges is always 20, since G = K5)\n",
    "\n",
    "    nodes = (\n",
    "        torch.sqrt(torch.sum(vel**2, dim=1)).unsqueeze(1).detach()\n",
    "    )  # norm of velocity vectors\n",
    "    rows, cols = edges\n",
    "    loc_dist = torch.sum((loc[rows] - loc[cols]) ** 2, 1).unsqueeze(\n",
    "        1\n",
    "    )  # relative distances among locations\n",
    "    edge_attr = torch.cat(\n",
    "        [edge_attr, loc_dist], 1\n",
    "    ).detach()  # concatenate all edge properties\n",
    "\n",
    "    return loc, vel, nodes, edges, edge_attr, charges, loc_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Without `equiadapt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonicalization_network = VNDeepSets(canon_hyperparams).to(device)\n",
    "prediction_network = GNN(pred_hyperparams).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\n",
    "                    \"params\": prediction_network.parameters(),\n",
    "                    \"lr\": hyperparams.learning_rate,\n",
    "                },\n",
    "                {\"params\": canonicalization_network.parameters(), \"lr\": hyperparams.learning_rate},\n",
    "            ]\n",
    "        )\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 30/30 [00:01<00:00, 15.88it/s, task_loss=1.99, loss=1.99]\n",
      "Epoch 1: 100%|██████████| 30/30 [00:02<00:00, 14.98it/s, task_loss=0.156, loss=0.156]\n",
      "Epoch 2: 100%|██████████| 30/30 [00:02<00:00, 12.26it/s, task_loss=0.0819, loss=0.0819]\n",
      "Epoch 3: 100%|██████████| 30/30 [00:02<00:00, 10.37it/s, task_loss=0.0717, loss=0.0717]\n",
      "Epoch 4: 100%|██████████| 30/30 [00:02<00:00, 10.86it/s, task_loss=0.0692, loss=0.0692]\n",
      "Epoch 5: 100%|██████████| 30/30 [00:02<00:00, 14.54it/s, task_loss=0.0663, loss=0.0663]\n",
      "Epoch 6: 100%|██████████| 30/30 [00:03<00:00,  9.71it/s, task_loss=0.0616, loss=0.0616]\n",
      "Epoch 7: 100%|██████████| 30/30 [00:01<00:00, 15.01it/s, task_loss=0.0614, loss=0.0614]\n",
      "Epoch 8: 100%|██████████| 30/30 [00:02<00:00, 13.51it/s, task_loss=0.065, loss=0.065]  \n",
      "Epoch 9: 100%|██████████| 30/30 [00:02<00:00, 12.67it/s, task_loss=0.0527, loss=0.0527]\n",
      "Epoch 10: 100%|██████████| 30/30 [00:02<00:00, 14.25it/s, task_loss=0.0454, loss=0.0454]\n",
      "Epoch 11: 100%|██████████| 30/30 [00:01<00:00, 15.25it/s, task_loss=0.0421, loss=0.0421]\n",
      "Epoch 12: 100%|██████████| 30/30 [00:02<00:00, 13.32it/s, task_loss=0.0394, loss=0.0394]\n",
      "Epoch 13: 100%|██████████| 30/30 [00:02<00:00, 11.99it/s, task_loss=0.0371, loss=0.0371]\n",
      "Epoch 14: 100%|██████████| 30/30 [00:02<00:00, 13.31it/s, task_loss=0.0361, loss=0.0361]\n",
      "Epoch 15: 100%|██████████| 30/30 [00:02<00:00, 11.45it/s, task_loss=0.034, loss=0.034]  \n",
      "Epoch 16: 100%|██████████| 30/30 [00:02<00:00, 14.67it/s, task_loss=0.0325, loss=0.0325]\n",
      "Epoch 17: 100%|██████████| 30/30 [00:01<00:00, 15.99it/s, task_loss=0.0333, loss=0.0333]\n",
      "Epoch 18: 100%|██████████| 30/30 [00:01<00:00, 15.80it/s, task_loss=0.0323, loss=0.0323]\n",
      "Epoch 19: 100%|██████████| 30/30 [00:02<00:00, 14.05it/s, task_loss=0.0304, loss=0.0304]\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tqdm_bar = tqdm(enumerate(train_loader), desc=f\"Epoch {epoch}\", total=len(train_loader))\n",
    "                 \n",
    "    total_loss, total_task_loss, = 0.0, 0.0,\n",
    "    for batch_idx, batch in tqdm_bar:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        training_metrics = {}\n",
    "        loss = 0.0\n",
    "\n",
    "        batch = [b.to(device) for b in batch]\n",
    "\n",
    "        # Split batch into inputs and targets\n",
    "        loc, vel, nodes, edges, edge_attr, charges, loc_end = get_data(batch)\n",
    "\n",
    "        # ------------------- code starting here is replaced by equiadapt -------------------\n",
    "\n",
    "        # Obtain rotation and translation vectors for canonicalization\n",
    "        rotation_vectors, translation_vectors = canonicalization_network(nodes, loc, edges, vel, edge_attr, charges)\n",
    "        rotation_matrix = gram_schmidt(rotation_vectors)\n",
    "        rotation_matrix_inverse = rotation_matrix.transpose(1, 2)\n",
    "\n",
    "        # Canonicalize node locations\n",
    "        canonical_loc = (torch.bmm(loc[:, None, :], \n",
    "                                   rotation_matrix_inverse).squeeze()- torch.bmm(translation_vectors[:, None, :], \n",
    "                                   rotation_matrix_inverse).squeeze()\n",
    "        )\n",
    "        # Canonicalize node velocities\n",
    "        canonical_vel = torch.bmm(vel[:, None, :], rotation_matrix_inverse).squeeze() \n",
    "        # Make prediction using canonical inputs \n",
    "        canonical_pred_loc = prediction_network(nodes, canonical_loc, edges, canonical_vel, edge_attr, charges)\n",
    "        # Un-canonicalize the predicted locations     \n",
    "        pred_loc = (torch.bmm(canonical_pred_loc[:, None, :], rotation_matrix).squeeze()+ translation_vectors)\n",
    "\n",
    "        # -----------------------------------------------------------------------------------\n",
    "\n",
    "        task_loss = loss_fn(pred_loc, loc_end)\n",
    "\n",
    "        loss += task_loss\n",
    "\n",
    "        # Logging the training metrics\n",
    "        total_loss += loss.item()\n",
    "        total_task_loss += task_loss.item()  \n",
    "        training_metrics.update({\n",
    "                \"task_loss\": total_task_loss / (batch_idx + 1),\n",
    "                \"loss\": total_loss / (batch_idx + 1),\n",
    "            })  \n",
    "        \n",
    "        # Usual training steps\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the training metrics\n",
    "        tqdm_bar.set_postfix(training_metrics)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with `equiadapt`\n",
    "Using `equiadapt`, we use an instance of `EuclideanGroupNBody`, which handles canonicalization and inverting canonicalization, using the `.canonicalize` and `invert_canonicalization` methods, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonicalization_network = VNDeepSets(canon_hyperparams)\n",
    "prediction_network = GNN(pred_hyperparams)\n",
    "canonicalizer = EuclideanGroupNBody(canonicalization_network, canon_hyperparams)\n",
    "optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\n",
    "                    \"params\": prediction_network.parameters(),\n",
    "                    \"lr\": hyperparams.learning_rate,\n",
    "                },\n",
    "                {\"params\": canonicalization_network.parameters(), \"lr\": hyperparams.learning_rate},\n",
    "            ]\n",
    "        )\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 30/30 [00:01<00:00, 15.27it/s, task_loss=1.82, loss=1.82]\n",
      "Epoch 1: 100%|██████████| 30/30 [00:02<00:00, 14.59it/s, task_loss=0.128, loss=0.128]\n",
      "Epoch 2: 100%|██████████| 30/30 [00:02<00:00, 13.11it/s, task_loss=0.0774, loss=0.0774]\n",
      "Epoch 3: 100%|██████████| 30/30 [00:02<00:00, 12.53it/s, task_loss=0.0698, loss=0.0698]\n",
      "Epoch 4: 100%|██████████| 30/30 [00:02<00:00, 14.04it/s, task_loss=0.0679, loss=0.0679]\n",
      "Epoch 5: 100%|██████████| 30/30 [00:02<00:00, 12.82it/s, task_loss=0.0754, loss=0.0754]\n",
      "Epoch 6: 100%|██████████| 30/30 [00:02<00:00, 13.56it/s, task_loss=0.0639, loss=0.0639]\n",
      "Epoch 7: 100%|██████████| 30/30 [00:02<00:00, 12.07it/s, task_loss=0.0603, loss=0.0603]\n",
      "Epoch 8: 100%|██████████| 30/30 [00:02<00:00, 13.41it/s, task_loss=0.0554, loss=0.0554]\n",
      "Epoch 9: 100%|██████████| 30/30 [00:01<00:00, 15.00it/s, task_loss=0.0514, loss=0.0514]\n",
      "Epoch 10: 100%|██████████| 30/30 [00:02<00:00, 12.85it/s, task_loss=0.0472, loss=0.0472]\n",
      "Epoch 11: 100%|██████████| 30/30 [00:02<00:00, 10.31it/s, task_loss=0.0429, loss=0.0429]\n",
      "Epoch 12: 100%|██████████| 30/30 [00:02<00:00, 13.50it/s, task_loss=0.0432, loss=0.0432]\n",
      "Epoch 13: 100%|██████████| 30/30 [00:02<00:00, 12.77it/s, task_loss=0.0424, loss=0.0424]\n",
      "Epoch 14: 100%|██████████| 30/30 [00:01<00:00, 16.99it/s, task_loss=0.0357, loss=0.0357]\n",
      "Epoch 15: 100%|██████████| 30/30 [00:02<00:00, 12.02it/s, task_loss=0.0327, loss=0.0327]\n",
      "Epoch 16: 100%|██████████| 30/30 [00:02<00:00, 13.01it/s, task_loss=0.032, loss=0.032]  \n",
      "Epoch 17: 100%|██████████| 30/30 [00:02<00:00, 13.76it/s, task_loss=0.0308, loss=0.0308]\n",
      "Epoch 18: 100%|██████████| 30/30 [00:01<00:00, 15.62it/s, task_loss=0.0298, loss=0.0298]\n",
      "Epoch 19: 100%|██████████| 30/30 [00:01<00:00, 16.15it/s, task_loss=0.0302, loss=0.0302]\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tqdm_bar = tqdm(enumerate(train_loader), desc=f\"Epoch {epoch}\", total=len(train_loader))\n",
    "                 \n",
    "    total_loss, total_task_loss, = 0.0, 0.0,\n",
    "    for batch_idx, batch in tqdm_bar:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        training_metrics = {}\n",
    "        loss = 0.0\n",
    "\n",
    "        batch = [b.to(device) for b in batch]\n",
    "\n",
    "        loc, vel, nodes, edges, edge_attr, charges, loc_end = get_data(batch)\n",
    "\n",
    "        ## ------------------- equiadapt code -------------------\n",
    "\n",
    "        # canonicalize the input data\n",
    "        canonical_loc, canonical_vel = canonicalizer(x=nodes, targets=None, loc=loc, edges=edges, vel=vel, edge_attr=edge_attr, charges=charges,)  \n",
    "        canonical_pred_loc = prediction_network(nodes, canonical_loc, edges, canonical_vel, edge_attr, charges)  # predict the output\n",
    "        pred_loc = canonicalizer.invert_canonicalization(canonical_pred_loc)  # invert the canonicalization\n",
    "\n",
    "        ## -----------------------------------------------------\n",
    "\n",
    "\n",
    "        task_loss = loss_fn(pred_loc, loc_end)\n",
    "\n",
    "        loss += task_loss\n",
    "\n",
    "        # Logging the training metrics\n",
    "        total_loss += loss.item()\n",
    "        total_task_loss += task_loss.item()  \n",
    "        training_metrics.update({\n",
    "                \"task_loss\": total_task_loss / (batch_idx + 1),\n",
    "                \"loss\": total_loss / (batch_idx + 1),\n",
    "            })  \n",
    "        \n",
    "        # Usual training steps\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the training metrics\n",
    "        tqdm_bar.set_postfix(training_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "equiadapt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
