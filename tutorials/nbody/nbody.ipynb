{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Body Experiment\n",
    "The goal of this notebook is to demonstrate how a workflow can be simplified using equiadapt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from equiadapt.nbody.canonicalization.euclidean_group import EuclideanGroupNBody\n",
    "from equiadapt.nbody.canonicalization_networks.custom_equivariant_networks import VNDeepSets\n",
    "from equiadapt.common.utils import gram_schmidt\n",
    "\n",
    "from examples.nbody.networks.euclideangraph_base_models import GNN\n",
    "from examples.nbody.prepare.nbody_data import NBodyDataModule\n",
    "from examples.nbody.model_utils import get_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameters:\n",
    "    def __init__(self):\n",
    "        self.model = \"NBodyPipeline\"\n",
    "        self.canon_model_type = \"vndeepsets\"\n",
    "        self.pred_model_type = \"Transformer\"\n",
    "        self.batch_size = 100\n",
    "        self.dryrun = False\n",
    "        self.use_wandb = False\n",
    "        self.checkpoint = False\n",
    "        self.num_epochs = 1000\n",
    "        self.num_workers = 0\n",
    "        self.auto_tune = False\n",
    "        self.seed = 0\n",
    "        self.learning_rate = 1e-3\n",
    "        self.weight_decay = 1e-12\n",
    "        self.patience = 1000\n",
    "\n",
    "class CanonicalizationHyperparameters:\n",
    "    def __init__(self):\n",
    "        self.architecture = \"vndeepsets\"\n",
    "        self.num_layers = 4\n",
    "        self.hidden_dim = 16\n",
    "        self.layer_pooling = \"mean\"\n",
    "        self.final_pooling = \"mean\"\n",
    "        self.out_dim = 4\n",
    "        self.batch_size = 100\n",
    "        self.nonlinearity = \"relu\"\n",
    "        self.canon_feature = \"p\"\n",
    "        self.canon_translation = False\n",
    "        self.angular_feature = \"pv\"\n",
    "        self.dropout = 0.5\n",
    "\n",
    "class PredictionHyperparameters:\n",
    "    def __init__(self):\n",
    "        self.architecture = \"GNN\"\n",
    "        self.num_layers = 4\n",
    "        self.hidden_dim = 32\n",
    "        self.input_dim = 6\n",
    "        self.in_node_nf = 1\n",
    "        self.in_edge_nf = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = Hyperparameters()\n",
    "canon_hyperparams = CanonicalizationHyperparameters()\n",
    "pred_hyperparams = PredictionHyperparameters()\n",
    "hyperparams.canon_hyperparams = canon_hyperparams\n",
    "hyperparams.pred_hyperparams = pred_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jikael/mcgill/comp396/equivariant-adaptation/EquivariantAdaptation/examples/nbody/prepare/nbody_data.py:114: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1670076225403/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  edge_attr = torch.Tensor(edge_attr).transpose(0, 1).unsqueeze(2)\n"
     ]
    }
   ],
   "source": [
    "nbody_data = NBodyDataModule(hyperparams)\n",
    "nbody_data.setup()\n",
    "train_loader = nbody_data.train_dataloader()\n",
    "\n",
    "nbody_data.setup(stage=\"test\")\n",
    "test_loader = nbody_data.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the batch into location features, velocity features, \n",
    "# node features, edges, edge features, charges, and end locations (ie. targets)\n",
    "def get_data(batch):\n",
    "    batch_size, n_nodes, _ = batch[0].size()\n",
    "    batch = [d.view(-1, d.size(2)) for d in batch]  # converts to 2D matrices\n",
    "    loc, vel, edge_attr, charges, loc_end = batch\n",
    "    edges = get_edges(\n",
    "        batch_size, n_nodes\n",
    "    )  # returns a list of two tensors, each of size num_edges * batch_size (where num_edges is always 20, since G = K5)\n",
    "\n",
    "    nodes = (\n",
    "        torch.sqrt(torch.sum(vel**2, dim=1)).unsqueeze(1).detach()\n",
    "    )  # norm of velocity vectors\n",
    "    rows, cols = edges\n",
    "    loc_dist = torch.sum((loc[rows] - loc[cols]) ** 2, 1).unsqueeze(\n",
    "        1\n",
    "    )  # relative distances among locations\n",
    "    edge_attr = torch.cat(\n",
    "        [edge_attr, loc_dist], 1\n",
    "    ).detach()  # concatenate all edge properties\n",
    "\n",
    "    return loc, vel, nodes, edges, edge_attr, charges, loc_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Without `equiadapt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonicalization_network = VNDeepSets(canon_hyperparams).to(device)\n",
    "prediction_network = GNN(pred_hyperparams).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\n",
    "                    \"params\": prediction_network.parameters(),\n",
    "                    \"lr\": hyperparams.learning_rate,\n",
    "                },\n",
    "                {\"params\": canonicalization_network.parameters(), \"lr\": hyperparams.learning_rate},\n",
    "            ]\n",
    "        )\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 30/30 [00:01<00:00, 15.67it/s, task_loss=1.89, loss=1.89]\n",
      "Epoch 1: 100%|██████████| 30/30 [00:01<00:00, 17.58it/s, task_loss=0.135, loss=0.135]\n",
      "Epoch 2: 100%|██████████| 30/30 [00:01<00:00, 16.49it/s, task_loss=0.0769, loss=0.0769]\n",
      "Epoch 3: 100%|██████████| 30/30 [00:01<00:00, 16.13it/s, task_loss=0.0711, loss=0.0711]\n",
      "Epoch 4: 100%|██████████| 30/30 [00:01<00:00, 15.57it/s, task_loss=0.0677, loss=0.0677]\n",
      "Epoch 5: 100%|██████████| 30/30 [00:01<00:00, 16.25it/s, task_loss=0.065, loss=0.065]  \n",
      "Epoch 6: 100%|██████████| 30/30 [00:02<00:00, 10.09it/s, task_loss=0.0626, loss=0.0626]\n",
      "Epoch 7: 100%|██████████| 30/30 [00:02<00:00, 14.06it/s, task_loss=0.0619, loss=0.0619]\n",
      "Epoch 8: 100%|██████████| 30/30 [00:03<00:00,  9.61it/s, task_loss=0.0571, loss=0.0571]\n",
      "Epoch 9: 100%|██████████| 30/30 [00:01<00:00, 16.93it/s, task_loss=0.0527, loss=0.0527]\n",
      "Epoch 10: 100%|██████████| 30/30 [00:02<00:00, 11.15it/s, task_loss=0.0502, loss=0.0502]\n",
      "Epoch 11: 100%|██████████| 30/30 [00:02<00:00, 12.74it/s, task_loss=0.0451, loss=0.0451]\n",
      "Epoch 12: 100%|██████████| 30/30 [00:01<00:00, 16.91it/s, task_loss=0.0431, loss=0.0431]\n",
      "Epoch 13: 100%|██████████| 30/30 [00:01<00:00, 16.54it/s, task_loss=0.0418, loss=0.0418]\n",
      "Epoch 14: 100%|██████████| 30/30 [00:01<00:00, 15.43it/s, task_loss=0.0373, loss=0.0373]\n",
      "Epoch 15: 100%|██████████| 30/30 [00:02<00:00, 14.84it/s, task_loss=0.0333, loss=0.0333]\n",
      "Epoch 16: 100%|██████████| 30/30 [00:02<00:00, 12.37it/s, task_loss=0.0323, loss=0.0323]\n",
      "Epoch 17: 100%|██████████| 30/30 [00:02<00:00, 14.64it/s, task_loss=0.0305, loss=0.0305]\n",
      "Epoch 18: 100%|██████████| 30/30 [00:02<00:00, 13.47it/s, task_loss=0.0292, loss=0.0292]\n",
      "Epoch 19: 100%|██████████| 30/30 [00:01<00:00, 16.29it/s, task_loss=0.0301, loss=0.0301]\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tqdm_bar = tqdm(enumerate(train_loader), desc=f\"Epoch {epoch}\", total=len(train_loader))\n",
    "                 \n",
    "    total_loss, total_task_loss, = 0.0, 0.0,\n",
    "    for batch_idx, batch in tqdm_bar:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        training_metrics = {}\n",
    "        loss = 0.0\n",
    "\n",
    "        batch = [b.to(device) for b in batch]\n",
    "\n",
    "        # Split batch into inputs and targets\n",
    "        loc, vel, nodes, edges, edge_attr, charges, loc_end = get_data(batch)\n",
    "\n",
    "        # ------------------- code starting here is replaced by equiadapt -------------------\n",
    "\n",
    "        # Obtain rotation and translation vectors for canonicalization\n",
    "        rotation_vectors, translation_vectors = canonicalization_network(nodes, loc, edges, vel, edge_attr, charges)\n",
    "        rotation_matrix = gram_schmidt(rotation_vectors)\n",
    "        rotation_matrix_inverse = rotation_matrix.transpose(1, 2)\n",
    "\n",
    "        # Canonicalize node locations\n",
    "        canonical_loc = (torch.bmm(loc[:, None, :], \n",
    "                                   rotation_matrix_inverse).squeeze()- torch.bmm(translation_vectors[:, None, :], \n",
    "                                   rotation_matrix_inverse).squeeze()\n",
    "        )\n",
    "        # Canonicalize node velocities\n",
    "        canonical_vel = torch.bmm(vel[:, None, :], rotation_matrix_inverse).squeeze() \n",
    "        # Make prediction using canonical inputs \n",
    "        canonical_pred_loc = prediction_network(nodes, canonical_loc, edges, canonical_vel, edge_attr, charges)\n",
    "        # Un-canonicalize the predicted locations     \n",
    "        pred_loc = (torch.bmm(canonical_pred_loc[:, None, :], rotation_matrix).squeeze()+ translation_vectors)\n",
    "\n",
    "        # -----------------------------------------------------------------------------------\n",
    "\n",
    "        task_loss = loss_fn(pred_loc, loc_end)\n",
    "\n",
    "        loss += task_loss\n",
    "\n",
    "        # Logging the training metrics\n",
    "        total_loss += loss.item()\n",
    "        total_task_loss += task_loss.item()  \n",
    "        training_metrics.update({\n",
    "                \"task_loss\": total_task_loss / (batch_idx + 1),\n",
    "                \"loss\": total_loss / (batch_idx + 1),\n",
    "            })  \n",
    "        \n",
    "        # Usual training steps\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the training metrics\n",
    "        tqdm_bar.set_postfix(training_metrics)\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with `equiadapt`\n",
    "Using `equiadapt`, we use an instance of `EuclideanGroupNBody`, which handles canonicalization and inverting canonicalization, using the `.canonicalize` and `invert_canonicalization` methods, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "canonicalization_network = VNDeepSets(canon_hyperparams)\n",
    "prediction_network = GNN(pred_hyperparams)\n",
    "canonicalizer = EuclideanGroupNBody(canonicalization_network, canon_hyperparams)\n",
    "optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\n",
    "                    \"params\": prediction_network.parameters(),\n",
    "                    \"lr\": hyperparams.learning_rate,\n",
    "                },\n",
    "                {\"params\": canonicalization_network.parameters(), \"lr\": hyperparams.learning_rate},\n",
    "            ]\n",
    "        )\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 30/30 [00:02<00:00, 14.93it/s, task_loss=1.63, loss=1.63]\n",
      "Epoch 1: 100%|██████████| 30/30 [00:01<00:00, 16.78it/s, task_loss=0.132, loss=0.132]\n",
      "Epoch 2: 100%|██████████| 30/30 [00:02<00:00, 14.36it/s, task_loss=0.079, loss=0.079]  \n",
      "Epoch 3: 100%|██████████| 30/30 [00:01<00:00, 15.51it/s, task_loss=0.0703, loss=0.0703]\n",
      "Epoch 4: 100%|██████████| 30/30 [00:02<00:00, 10.34it/s, task_loss=0.0675, loss=0.0675]\n",
      "Epoch 5: 100%|██████████| 30/30 [00:01<00:00, 17.05it/s, task_loss=0.065, loss=0.065]  \n",
      "Epoch 6: 100%|██████████| 30/30 [00:03<00:00,  9.92it/s, task_loss=0.0624, loss=0.0624]\n",
      "Epoch 7: 100%|██████████| 30/30 [00:01<00:00, 16.50it/s, task_loss=0.0604, loss=0.0604]\n",
      "Epoch 8: 100%|██████████| 30/30 [00:01<00:00, 16.10it/s, task_loss=0.0591, loss=0.0591]\n",
      "Epoch 9: 100%|██████████| 30/30 [00:02<00:00, 14.37it/s, task_loss=0.0542, loss=0.0542]\n",
      "Epoch 10: 100%|██████████| 30/30 [00:02<00:00, 12.92it/s, task_loss=0.0467, loss=0.0467]\n",
      "Epoch 11: 100%|██████████| 30/30 [00:02<00:00, 12.71it/s, task_loss=0.043, loss=0.043]  \n",
      "Epoch 12: 100%|██████████| 30/30 [00:02<00:00, 11.36it/s, task_loss=0.0435, loss=0.0435]\n",
      "Epoch 13: 100%|██████████| 30/30 [00:02<00:00, 13.87it/s, task_loss=0.0362, loss=0.0362]\n",
      "Epoch 14: 100%|██████████| 30/30 [00:02<00:00, 10.66it/s, task_loss=0.0342, loss=0.0342]\n",
      "Epoch 15: 100%|██████████| 30/30 [00:02<00:00, 13.40it/s, task_loss=0.0328, loss=0.0328]\n",
      "Epoch 16: 100%|██████████| 30/30 [00:01<00:00, 17.59it/s, task_loss=0.0315, loss=0.0315]\n",
      "Epoch 17: 100%|██████████| 30/30 [00:02<00:00, 13.15it/s, task_loss=0.0303, loss=0.0303]\n",
      "Epoch 18: 100%|██████████| 30/30 [00:03<00:00,  9.49it/s, task_loss=0.029, loss=0.029]  \n",
      "Epoch 19: 100%|██████████| 30/30 [00:02<00:00, 12.47it/s, task_loss=0.0285, loss=0.0285]\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tqdm_bar = tqdm(enumerate(train_loader), desc=f\"Epoch {epoch}\", total=len(train_loader))\n",
    "                 \n",
    "    total_loss, total_task_loss, = 0.0, 0.0,\n",
    "    for batch_idx, batch in tqdm_bar:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        training_metrics = {}\n",
    "        loss = 0.0\n",
    "\n",
    "        batch = [b.to(device) for b in batch]\n",
    "\n",
    "        loc, vel, nodes, edges, edge_attr, charges, loc_end = get_data(batch)\n",
    "\n",
    "        ## ------------------- equiadapt code -------------------\n",
    "\n",
    "        # canonicalize the input data\n",
    "        canonical_loc, canonical_vel = canonicalizer(x=nodes, targets=None, loc=loc, edges=edges, vel=vel, edge_attr=edge_attr, charges=charges,)  \n",
    "        canonical_pred_loc = prediction_network(nodes, canonical_loc, edges, canonical_vel, edge_attr, charges)  # predict the output\n",
    "        pred_loc = canonicalizer.invert_canonicalization(canonical_pred_loc)  # invert the canonicalization\n",
    "\n",
    "        ## -----------------------------------------------------\n",
    "\n",
    "\n",
    "        task_loss = loss_fn(pred_loc, loc_end)\n",
    "\n",
    "        loss += task_loss\n",
    "\n",
    "        # Logging the training metrics\n",
    "        total_loss += loss.item()\n",
    "        total_task_loss += task_loss.item()  \n",
    "        training_metrics.update({\n",
    "                \"task_loss\": total_task_loss / (batch_idx + 1),\n",
    "                \"loss\": total_loss / (batch_idx + 1),\n",
    "            })  \n",
    "        \n",
    "        # Usual training steps\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the training metrics\n",
    "        tqdm_bar.set_postfix(training_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "equiadapt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
