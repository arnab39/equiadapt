{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Equivariant canonicalization for an Invariant Task (Image Classification with ViT-Base)\n",
    "In this notebook, we test whether the group equivariant image canonicalizer can generate a canonical orientation properly for sample images which can be processed by the prediction network. We also visualize the ground truth and predicted class from a prediction network, which is Vision Transformer ([Dosovitskiy et. al, 2020](https://arxiv.org/abs/2010.11929)). Further we consider the group to be $C_4$ which is rotation of 4 discrete rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from equiadapt.images.canonicalization_networks.escnn_networks import ESCNNEquivariantNetwork\n",
    "from equiadapt.images.canonicalization.discrete_group import GroupEquivariantImageCanonicalization\n",
    "from equiadapt.common.basecanonicalization import IdentityCanonicalization\n",
    "\n",
    "from examples.images.classification.prepare import STL10DataModule\n",
    "from examples.images.classification.inference_utils import GroupInference\n",
    "from examples.images.classification.model_utils import get_prediction_network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Test dataset size:  8000\n"
     ]
    }
   ],
   "source": [
    "class DatasetHyperparams:\n",
    "    def __init__(self):\n",
    "        self.dataset_name = \"stl10\" # Name of the dataset to use\n",
    "        self.data_path = \"/home/mila/s/siba-smarak.panigrahi/scratch/data/stl10\" # Path to the dataset\n",
    "        self.augment = 1 # Whether to use data augmentation (1) or not (0)\n",
    "        self.num_workers = 4 # Number of workers for data loading\n",
    "        self.batch_size = 64 # Number of samples per batch\n",
    "        \n",
    "dataset_hyperparams = DatasetHyperparams()\n",
    "data = STL10DataModule(hyperparams=dataset_hyperparams)\n",
    "\n",
    "data.setup()\n",
    "train_loader = data.train_dataloader()\n",
    "\n",
    "data.setup(stage=\"test\")\n",
    "test_loader = data.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn, image_shape, num_classes = torch.nn.CrossEntropyLoss(), (3, 224, 224), 10\n",
    "\n",
    "# get the prediction network, which in this case is Vision Transformer\n",
    "prediction_network = get_prediction_network(\n",
    "    architecture = \"vit\", \n",
    "    dataset_name = \"stl10\",\n",
    "    use_pretrained = True,\n",
    "    freeze_encoder = False,\n",
    "    input_shape = image_shape,\n",
    "    num_classes = num_classes\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design canonicalization hyperparams class\n",
    "class CanonicalizationHyperparams:\n",
    "    def __init__(self):\n",
    "        self.canonicalization_type=\"group_equivariant\" # canonicalization type network\n",
    "        self.network_type = \"escnn\" # group equivariant canonicalization\n",
    "        self.resize_shape = 96 # resize shape for the canonicalization network\n",
    "        self.network_hyperparams = {\n",
    "            \"kernel_size\": 5, # Kernel size for the canonization network\n",
    "            \"out_channels\": 32, # Number of output channels for the canonization network\n",
    "            \"num_layers\": 3, # Number of layers in the canonization network\n",
    "            \"group_type\": \"rotation\", # Type of group for the canonization network\n",
    "            \"num_rotations\": 4, # Number of rotations for the canonization network \n",
    "        }\n",
    "        self.beta = 1.0 \n",
    "        self.input_crop_ratio = 0.8\n",
    "        \n",
    "canonicalization_hyperparams = CanonicalizationHyperparams()\n",
    "\n",
    "canonicalization_network = ESCNNEquivariantNetwork(\n",
    "                        image_shape,\n",
    "                        **canonicalization_hyperparams.network_hyperparams,\n",
    "                    ).to(device)\n",
    "\n",
    "canonicalizer = GroupEquivariantImageCanonicalization(\n",
    "            canonicalization_network,\n",
    "            canonicalization_hyperparams,\n",
    "            image_shape\n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceHyperparams:\n",
    "    def __init__(self):\n",
    "        self.method = \"group\"\n",
    "        self.group_type = \"rotation\"\n",
    "        self.num_rotations = 4\n",
    "        \n",
    "inference_method = GroupInference(\n",
    "        canonicalizer,\n",
    "        prediction_network,\n",
    "        num_classes,\n",
    "        InferenceHyperparams(),\n",
    "        image_shape\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning ViT on STL10 with a $C_4$ equivariant canonicalization network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW([\n",
    "        {'params': prediction_network.parameters(), 'lr': 1e-5},\n",
    "        {'params': canonicalizer.parameters(), 'lr': 1e-3},\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/79 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 79/79 [00:42<00:00,  1.85it/s, acc=0.623, task_loss=1.42, prior_loss=1.21, loss=123]\n",
      "Epoch 1: 100%|██████████| 79/79 [00:41<00:00,  1.90it/s, acc=0.928, task_loss=0.348, prior_loss=1.1, loss=111] \n",
      "Epoch 2: 100%|██████████| 79/79 [00:41<00:00,  1.90it/s, acc=0.962, task_loss=0.168, prior_loss=1.05, loss=105]\n",
      "Epoch 3: 100%|██████████| 79/79 [00:41<00:00,  1.89it/s, acc=0.976, task_loss=0.109, prior_loss=1.01, loss=101]  \n",
      "Epoch 4: 100%|██████████| 79/79 [00:41<00:00,  1.90it/s, acc=0.985, task_loss=0.0737, prior_loss=0.984, loss=98.5]\n",
      "Epoch 5: 100%|██████████| 79/79 [00:41<00:00,  1.89it/s, acc=0.988, task_loss=0.0582, prior_loss=0.953, loss=95.4]\n",
      "Epoch 6: 100%|██████████| 79/79 [00:41<00:00,  1.90it/s, acc=0.989, task_loss=0.0483, prior_loss=0.937, loss=93.7]\n",
      "Epoch 7: 100%|██████████| 79/79 [00:41<00:00,  1.90it/s, acc=0.994, task_loss=0.0312, prior_loss=0.905, loss=90.5]\n",
      "Epoch 8: 100%|██████████| 79/79 [00:41<00:00,  1.88it/s, acc=0.994, task_loss=0.0308, prior_loss=0.892, loss=89.2]\n",
      "Epoch 9: 100%|██████████| 79/79 [00:41<00:00,  1.90it/s, acc=0.996, task_loss=0.0239, prior_loss=0.881, loss=88.1]\n",
      "Epoch 10: 100%|██████████| 79/79 [00:41<00:00,  1.90it/s, acc=0.995, task_loss=0.0226, prior_loss=0.871, loss=87.1]\n",
      "Epoch 11: 100%|██████████| 79/79 [00:41<00:00,  1.90it/s, acc=0.994, task_loss=0.0268, prior_loss=0.856, loss=85.6]\n",
      "Epoch 12: 100%|██████████| 79/79 [00:41<00:00,  1.90it/s, acc=0.994, task_loss=0.0216, prior_loss=0.837, loss=83.8]\n",
      "Epoch 13: 100%|██████████| 79/79 [00:41<00:00,  1.90it/s, acc=0.994, task_loss=0.0279, prior_loss=0.819, loss=82]  \n",
      "Epoch 14: 100%|██████████| 79/79 [00:41<00:00,  1.90it/s, acc=0.997, task_loss=0.0139, prior_loss=0.817, loss=81.7]\n",
      "Epoch 15: 100%|██████████| 79/79 [00:41<00:00,  1.88it/s, acc=0.997, task_loss=0.0125, prior_loss=0.81, loss=81]   \n",
      "Epoch 16: 100%|██████████| 79/79 [00:41<00:00,  1.91it/s, acc=0.997, task_loss=0.0102, prior_loss=0.797, loss=79.7] \n",
      "Epoch 17: 100%|██████████| 79/79 [00:41<00:00,  1.89it/s, acc=0.998, task_loss=0.00984, prior_loss=0.793, loss=79.3]\n",
      "Epoch 18: 100%|██████████| 79/79 [00:41<00:00,  1.89it/s, acc=0.998, task_loss=0.0113, prior_loss=0.796, loss=79.6] \n",
      "Epoch 19: 100%|██████████| 79/79 [00:41<00:00,  1.90it/s, acc=0.998, task_loss=0.0088, prior_loss=0.798, loss=79.8] \n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "# finetuning the prediction network with the canonicalizer for STL10 dataset\n",
    "for epoch in range(epochs):\n",
    "    tqdm_bar = tqdm(enumerate(train_loader), desc=f\"Epoch {epoch}\", total=len(train_loader))\n",
    "                 \n",
    "    total_acc, total_loss, total_task_loss, total_prior_loss = 0.0, 0.0, 0.0, 0.0\n",
    "    for batch_idx, batch in tqdm_bar:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        batch_size, num_channels, height, width = x.shape\n",
    "        assert (num_channels, height, width) == image_shape\n",
    "\n",
    "        training_metrics = {}\n",
    "        loss = 0.0\n",
    "        \n",
    "        # canonicalize the input data\n",
    "        x_canonicalized = canonicalizer(x)\n",
    "        \n",
    "        # Get the predictions from the prediction network\n",
    "        logits = prediction_network(x_canonicalized)\n",
    "            \n",
    "        # Evaluate the task loss\n",
    "        task_loss = loss_fn(logits, y)\n",
    "        loss += task_loss\n",
    "        \n",
    "        # Add prior regularization loss\n",
    "        prior_loss = canonicalizer.get_prior_regularization_loss()\n",
    "        loss += prior_loss * 100\n",
    "            \n",
    "        # Get the predictions and calculate the accuracy \n",
    "        preds = logits.argmax(dim=-1)\n",
    "        acc = (preds == y).float().mean()\n",
    "            \n",
    "            \n",
    "        # Logging the training metrics\n",
    "        total_acc += acc.item()\n",
    "        total_loss += loss.item()\n",
    "        total_task_loss += task_loss.item()\n",
    "        total_prior_loss += prior_loss.item()   \n",
    "        training_metrics.update({\n",
    "                \"acc\": total_acc / (batch_idx + 1),\n",
    "                \"task_loss\": total_task_loss / (batch_idx + 1),\n",
    "                \"prior_loss\": total_prior_loss / (batch_idx + 1), \n",
    "                \"loss\": total_loss / (batch_idx + 1),\n",
    "            })  \n",
    "        \n",
    "        # Usual training steps\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the training metrics\n",
    "        tqdm_bar.set_postfix(training_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 125/125 [01:23<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.971\n",
      "Test Group Accuracy: 0.971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_tqdm_bar = tqdm(enumerate(test_loader), desc=f\"Testing\", total=len(test_loader))\n",
    "total_acc, total_group_acc = 0, 0\n",
    "for batch_idx, batch in test_tqdm_bar:\n",
    "    x, y = batch\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    batch_size, num_channels, height, width = x.shape\n",
    "    assert (num_channels, height, width) == image_shape\n",
    "\n",
    "    test_metrics = inference_method.get_inference_metrics(x, y)\n",
    "    \n",
    "    total_acc += test_metrics[\"test/acc\"]\n",
    "    total_group_acc += test_metrics[\"test/group_acc\"]\n",
    "    \n",
    "    \n",
    "print(f\"Test Accuracy: {total_acc/len(test_loader):.3f}\")\n",
    "print(f\"Test Group Accuracy: {total_group_acc/len(test_loader):.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning ViT on STL10 without canonicalization network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 79/79 [00:34<00:00,  2.31it/s, acc=0.758, task_loss=1.12, loss=1.12]\n",
      "Epoch 1: 100%|██████████| 79/79 [00:34<00:00,  2.31it/s, acc=0.974, task_loss=0.166, loss=0.166]\n",
      "Epoch 2: 100%|██████████| 79/79 [00:34<00:00,  2.32it/s, acc=0.989, task_loss=0.0713, loss=0.0713]\n",
      "Epoch 3: 100%|██████████| 79/79 [00:34<00:00,  2.32it/s, acc=0.993, task_loss=0.0438, loss=0.0438]\n",
      "Epoch 4: 100%|██████████| 79/79 [00:33<00:00,  2.33it/s, acc=0.997, task_loss=0.0266, loss=0.0266]\n",
      "Epoch 5: 100%|██████████| 79/79 [00:33<00:00,  2.32it/s, acc=0.999, task_loss=0.0144, loss=0.0144]\n",
      "Epoch 6: 100%|██████████| 79/79 [00:34<00:00,  2.32it/s, acc=0.999, task_loss=0.0121, loss=0.0121]\n",
      "Epoch 7: 100%|██████████| 79/79 [00:34<00:00,  2.32it/s, acc=0.999, task_loss=0.0119, loss=0.0119]  \n",
      "Epoch 8: 100%|██████████| 79/79 [00:34<00:00,  2.31it/s, acc=1, task_loss=0.00626, loss=0.00626]\n",
      "Epoch 9: 100%|██████████| 79/79 [00:34<00:00,  2.31it/s, acc=1, task_loss=0.00488, loss=0.00488]\n",
      "Epoch 10: 100%|██████████| 79/79 [00:34<00:00,  2.32it/s, acc=1, task_loss=0.00403, loss=0.00403]\n",
      "Epoch 11: 100%|██████████| 79/79 [00:34<00:00,  2.30it/s, acc=1, task_loss=0.00384, loss=0.00384]    \n",
      "Epoch 12: 100%|██████████| 79/79 [00:34<00:00,  2.29it/s, acc=1, task_loss=0.0031, loss=0.0031]  \n",
      "Epoch 13: 100%|██████████| 79/79 [00:34<00:00,  2.32it/s, acc=0.999, task_loss=0.00599, loss=0.00599]\n",
      "Epoch 14: 100%|██████████| 79/79 [00:34<00:00,  2.32it/s, acc=1, task_loss=0.00332, loss=0.00332]\n",
      "Epoch 15: 100%|██████████| 79/79 [00:34<00:00,  2.31it/s, acc=1, task_loss=0.00226, loss=0.00226]\n",
      "Epoch 16: 100%|██████████| 79/79 [00:34<00:00,  2.30it/s, acc=0.997, task_loss=0.0117, loss=0.0117]  \n",
      "Epoch 17: 100%|██████████| 79/79 [00:34<00:00,  2.30it/s, acc=0.999, task_loss=0.00452, loss=0.00452]\n",
      "Epoch 18: 100%|██████████| 79/79 [00:34<00:00,  2.32it/s, acc=0.999, task_loss=0.00335, loss=0.00335]\n",
      "Epoch 19: 100%|██████████| 79/79 [00:34<00:00,  2.31it/s, acc=0.999, task_loss=0.00595, loss=0.00595]\n"
     ]
    }
   ],
   "source": [
    "# redefine the prediction network (to reset the weights)\n",
    "prediction_network = get_prediction_network(\n",
    "    architecture = \"vit\",\n",
    "    dataset_name = \"stl10\",\n",
    "    use_pretrained = True,\n",
    "    freeze_encoder = False,\n",
    "    input_shape = image_shape,\n",
    "    num_classes = num_classes\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "optimizer = torch.optim.AdamW([\n",
    "        {'params': prediction_network.parameters(), 'lr': 1e-5},\n",
    "    ])\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "# finetuning the prediction network with the canonicalizer for STL10 dataset\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tqdm_bar = tqdm(enumerate(train_loader), desc=f\"Epoch {epoch}\", total=len(train_loader))\n",
    "                 \n",
    "    total_acc, total_loss, total_task_loss = 0.0, 0.0, 0.0\n",
    "    for batch_idx, batch in tqdm_bar:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        batch_size, num_channels, height, width = x.shape\n",
    "        assert (num_channels, height, width) == image_shape\n",
    "\n",
    "        training_metrics = {}\n",
    "        loss = 0.0\n",
    "        \n",
    "        # Get the predictions from the prediction network\n",
    "        logits = prediction_network(x)\n",
    "            \n",
    "        # Evaluate the task loss\n",
    "        task_loss = loss_fn(logits, y)\n",
    "        loss += task_loss\n",
    "            \n",
    "        # Get the predictions and calculate the accuracy \n",
    "        preds = logits.argmax(dim=-1)\n",
    "        acc = (preds == y).float().mean()\n",
    "            \n",
    "            \n",
    "        # Logging the training metrics\n",
    "        total_acc += acc.item()\n",
    "        total_loss += loss.item()\n",
    "        total_task_loss += task_loss.item()\n",
    "        training_metrics.update({\n",
    "                \"acc\": total_acc / (batch_idx + 1),\n",
    "                \"task_loss\": total_task_loss / (batch_idx + 1),\n",
    "                \"loss\": total_loss / (batch_idx + 1),\n",
    "            })  \n",
    "        \n",
    "        # Usual training steps\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log the training metrics\n",
    "        tqdm_bar.set_postfix(training_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 125/125 [01:12<00:00,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.982\n",
      "Test Group Accuracy: 0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class InferenceHyperparams:\n",
    "    def __init__(self):\n",
    "        self.method = \"group\"\n",
    "        self.group_type = \"rotation\"\n",
    "        self.num_rotations = 4\n",
    "        \n",
    "inference_method = GroupInference(\n",
    "        IdentityCanonicalization(),\n",
    "        prediction_network,\n",
    "        num_classes,\n",
    "        InferenceHyperparams(),\n",
    "        image_shape\n",
    "    )\n",
    "\n",
    "test_tqdm_bar = tqdm(enumerate(test_loader), desc=f\"Testing\", total=len(test_loader))\n",
    "total_acc, total_group_acc = 0, 0\n",
    "for batch_idx, batch in test_tqdm_bar:\n",
    "    x, y = batch\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    batch_size, num_channels, height, width = x.shape\n",
    "    assert (num_channels, height, width) == image_shape\n",
    "\n",
    "    test_metrics = inference_method.get_inference_metrics(x, y)\n",
    "    \n",
    "    total_acc += test_metrics[\"test/acc\"]\n",
    "    total_group_acc += test_metrics[\"test/group_acc\"]\n",
    "    \n",
    "    \n",
    "print(f\"Test Accuracy: {total_acc/len(test_loader):.3f}\")\n",
    "print(f\"Test Group Accuracy: {total_group_acc/len(test_loader):.3f}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
